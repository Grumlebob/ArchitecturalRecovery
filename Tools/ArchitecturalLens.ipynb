{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65856278",
   "metadata": {},
   "source": [
    "# Architectural Lens: Advanced Architecture Recovery Tool\n",
    "\n",
    "This notebook implements an enhanced version of Architectural‑Lens to perform architectural recovery on the Zeeguu project. It analyzes the code in both `Data/api` (backend) and `Data/frontend` (frontend) to generate comprehensive architecture views."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f293f04",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "- **Module Dependency Visualization:** Identify core components and their interdependencies\n",
    "- **Architectural Pattern Detection:** Spot potential patterns and anti‑patterns in the design\n",
    "- **Impact Analysis:** Understand how changes in one module may affect the overall architecture\n",
    "- **Layered Architecture Analysis:** Detect and visualize layering violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceea1def",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Architectural-Lens'...\n",
      "hint: core.useBuiltinFSMonitor=true is deprecated;please set core.fsmonitor=true instead\n",
      "hint: Disable this message with \"git config set advice.useCoreFSMonitorConfig false\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: graphviz in c:\\users\\jgrum\\appdata\\roaming\\python\\python312\\site-packages (0.20.3)\n",
      "Requirement already satisfied: pydot in c:\\users\\jgrum\\appdata\\roaming\\python\\python312\\site-packages (3.0.4)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: pyparsing>=3.0.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydot) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Clone the Architectural‑Lens repository if not already cloned\n",
    "if not os.path.exists(\"Architectural-Lens\"):\n",
    "    !git clone https://github.com/Perlten/Architectural-Lens.git\n",
    "else:\n",
    "    print(\"Repository already cloned.\")\n",
    "\n",
    "# Install required dependencies\n",
    "!pip install graphviz pydot networkx matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b462198",
   "metadata": {},
   "source": [
    "## Initialize Architectural‑Lens Configurations\n",
    "\n",
    "The following cell initializes the Architectural‑Lens configuration in both the backend and frontend directories. It changes to the respective project folder, runs `archlens init` (which creates an `archlens.json` file), and then returns to the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e96fa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Architectural‑Lens in: c:\\Programming\\Architecture\\ArchitecturalRecovery\\Data\\api\n",
      "\n",
      "api archlens.json content:\n",
      "{\n",
      "    \"$schema\": \"https://raw.githubusercontent.com/Perlten/Architectural-Lens/master/config.schema.json\",\n",
      "    \"name\": \"api\",\n",
      "    \"rootFolder\": \"\",\n",
      "    \"github\": {\n",
      "        \"url\": \"\",\n",
      "        \"branch\": \"main\"\n",
      "    },\n",
      "    \"saveLocation\": \"./diagrams/\",\n",
      "    \"views\": {\n",
      "        \"completeView\": {\n",
      "            \"packages\": [\n",
      "                {\n",
      "                    \"packagePath\": \"\",\n",
      "                    \"depth\": 0\n",
      "                }\n",
      "            ],\n",
      "            \"ignorePackages\": []\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Initializing Architectural‑Lens in: c:\\Programming\\Architecture\\ArchitecturalRecovery\\Data\\frontend\n",
      "\n",
      "frontend archlens.json content:\n",
      "{\n",
      "    \"$schema\": \"https://raw.githubusercontent.com/Perlten/Architectural-Lens/master/config.schema.json\",\n",
      "    \"name\": \"frontend\",\n",
      "    \"rootFolder\": \"\",\n",
      "    \"github\": {\n",
      "        \"url\": \"\",\n",
      "        \"branch\": \"main\"\n",
      "    },\n",
      "    \"saveLocation\": \"./diagrams/\",\n",
      "    \"views\": {\n",
      "        \"completeView\": {\n",
      "            \"packages\": [\n",
      "                {\n",
      "                    \"packagePath\": \"\",\n",
      "                    \"depth\": 0\n",
      "                }\n",
      "            ],\n",
      "            \"ignorePackages\": []\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def init_archlens(project):\n",
    "    # Compute the project folder path relative to the current working directory.\n",
    "    # Your current working directory is C:\\Programming\\Architecture\\ArchitecturalRecovery,\n",
    "    # so the project folders are in 'Data/api' and 'Data/frontend'.\n",
    "    project_folder = os.path.join('Data', project)\n",
    "    abs_project_folder = os.path.abspath(project_folder)\n",
    "    print(f\"Initializing Architectural‑Lens in: {abs_project_folder}\")\n",
    "    \n",
    "    # Check if the project folder exists\n",
    "    if not os.path.exists(project_folder):\n",
    "        print(f\"Error: The folder '{abs_project_folder}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Save current directory\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # Change to the project folder\n",
    "    os.chdir(project_folder)\n",
    "    \n",
    "    # Run the 'archlens init' command as a shell command.\n",
    "    # This uses the executable installed in your PATH.\n",
    "    result = subprocess.run(\"archlens init\", shell=True, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(\"Error running 'archlens init':\")\n",
    "        print(result.stderr)\n",
    "    else:\n",
    "        print(result.stdout)\n",
    "    \n",
    "    # Optionally, print the generated archlens.json to confirm it was created\n",
    "    config_file = 'archlens.json'\n",
    "    if os.path.exists(config_file):\n",
    "        print(f\"{project} archlens.json content:\")\n",
    "        with open(config_file, 'r') as f:\n",
    "            print(f.read())\n",
    "    else:\n",
    "        print(f\"archlens.json was not created in {abs_project_folder}.\")\n",
    "    \n",
    "    # Change back to the original directory\n",
    "    os.chdir(current_dir)\n",
    "\n",
    "# Initialize configuration for backend and frontend\n",
    "init_archlens('api')\n",
    "init_archlens('frontend')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f9fe4",
   "metadata": {},
   "source": [
    "## Basic Architecture Recovery\n",
    "First, we'll run a basic analysis to understand the overall structure by combining both projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ceeaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'c:\\\\Programming\\\\Architecture\\\\ArchitecturalRecovery\\\\Architectural-Lens\\\\archlens.py': [Errno 2] No such file or directory\n",
      "'dot' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Run the render command using the installed archlens command\n",
    "result = subprocess.run([\"archlens\", \"render\", \"--source\", \"Data/api\", \"Data/frontend\", \"--output\", \"basic_archlens_output.dot\"],\n",
    "                        capture_output=True, text=True)\n",
    "if result.returncode != 0:\n",
    "    print(\"Error running 'archlens render':\")\n",
    "    print(result.stderr)\n",
    "else:\n",
    "    print(result.stdout)\n",
    "\n",
    "# Run the dot command to convert the DOT file to PNG\n",
    "result_dot = subprocess.run([\"dot\", \"-Tpng\", \"basic_archlens_output.dot\", \"-o\", \"basic_archlens_output.png\"],\n",
    "                              capture_output=True, text=True)\n",
    "if result_dot.returncode != 0:\n",
    "    print(\"Error running 'dot' command:\")\n",
    "    print(result_dot.stderr)\n",
    "else:\n",
    "    print(\"Diagram generated successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1cd44",
   "metadata": {},
   "source": [
    "## Enhanced Analysis with Custom Filtering\n",
    "Let's create a more focused view by filtering out less important modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "156252c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'basic_archlens_output.dot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdrawing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnx_pydot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_dot\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the basic DOT file into a NetworkX graph\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m G \u001b[38;5;241m=\u001b[39m read_dot(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasic_archlens_output.dot\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Filter the graph to include only nodes with at least 3 incoming or outgoing edges\u001b[39;00m\n\u001b[0;32m      9\u001b[0m significant_nodes \u001b[38;5;241m=\u001b[39m [node \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m G\u001b[38;5;241m.\u001b[39mnodes() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(G\u001b[38;5;241m.\u001b[39mpredecessors(node))) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(G\u001b[38;5;241m.\u001b[39msuccessors(node))) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[1;32m<class 'networkx.utils.decorators.argmap'> compilation 5:3\u001b[0m, in \u001b[0;36margmap_read_dot_1\u001b[1;34m(path, backend, **backend_kwargs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbz2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\networkx\\utils\\decorators.py:193\u001b[0m, in \u001b[0;36mopen_file.<locals>._open_file\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# could be None, or a file handle, in which case the algorithm will deal with it\u001b[39;00m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m path, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m fobj \u001b[38;5;241m=\u001b[39m _dispatch_dict[ext](path, mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fobj, \u001b[38;5;28;01mlambda\u001b[39;00m: fobj\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'basic_archlens_output.dot'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.drawing.nx_pydot import read_dot\n",
    "\n",
    "# Load the basic DOT file into a NetworkX graph\n",
    "G = read_dot('basic_archlens_output.dot')\n",
    "\n",
    "# Filter the graph to include only nodes with at least 3 incoming or outgoing edges\n",
    "significant_nodes = [node for node in G.nodes() if len(list(G.predecessors(node))) + len(list(G.successors(node))) >= 3]\n",
    "G_filtered = G.subgraph(significant_nodes)\n",
    "\n",
    "# Save the filtered graph\n",
    "nx.drawing.nx_pydot.write_dot(G_filtered, 'filtered_archlens_output.dot')\n",
    "\n",
    "# Convert the filtered DOT file to PNG\n",
    "!dot -Tpng filtered_archlens_output.dot -o filtered_archlens_output.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0c968d",
   "metadata": {},
   "source": [
    "## Separate Backend and Frontend Analysis\n",
    "Analyzing the backend and frontend separately can provide clearer architectural insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Architectural‑Lens on just the backend\n",
    "!python Architectural-Lens/archlens.py --source ../Data/api --output api_archlens_output.dot\n",
    "!dot -Tpng api_archlens_output.dot -o api_archlens_output.png\n",
    "\n",
    "# Run Architectural‑Lens on just the frontend\n",
    "!python Architectural-Lens/archlens.py --source ../Data/frontend --output frontend_archlens_output.dot\n",
    "!dot -Tpng frontend_archlens_output.dot -o frontend_archlens_output.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Core Components\n",
    "Let's extract the most central modules in both the backend and frontend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_centrality(dot_file, title):\n",
    "    from networkx.drawing.nx_pydot import read_dot\n",
    "    import networkx as nx\n",
    "    \n",
    "    # Load the graph from the DOT file\n",
    "    G = read_dot(dot_file)\n",
    "    \n",
    "    # Calculate centrality measures\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "    \n",
    "    # Sort nodes by centrality\n",
    "    degree_sorted = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    betweenness_sorted = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(f\"\\n{title} - Top 10 modules by Degree Centrality:\")\n",
    "    for node, centrality in degree_sorted:\n",
    "        print(f\"{node}: {centrality:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{title} - Top 10 modules by Betweenness Centrality:\")\n",
    "    for node, centrality in betweenness_sorted:\n",
    "        print(f\"{node}: {centrality:.4f}\")\n",
    "    \n",
    "    return G, degree_centrality, betweenness_centrality\n",
    "\n",
    "# Analyze backend\n",
    "api_G, api_degree, api_between = analyze_centrality('api_archlens_output.dot', 'Backend (API)')\n",
    "\n",
    "# Analyze frontend\n",
    "frontend_G, frontend_degree, frontend_between = analyze_centrality('frontend_archlens_output.dot', 'Frontend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Core Components\n",
    "Create a subgraph of the most important components for a clearer architectural view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_core_components(G, degree_centrality, betweenness_centrality, output_file, title):\n",
    "    import networkx as nx\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Calculate combined centrality for each node\n",
    "    combined_centrality = {}\n",
    "    for node in G.nodes():\n",
    "        combined_centrality[node] = degree_centrality.get(node, 0) + betweenness_centrality.get(node, 0)\n",
    "    \n",
    "    # Sort and take top 20% of nodes\n",
    "    sorted_nodes = sorted(combined_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_nodes = [node for node, score in sorted_nodes[:max(1, int(len(sorted_nodes) * 0.2))]]\n",
    "    \n",
    "    # Create subgraph with the top nodes\n",
    "    core_G = G.subgraph(top_nodes)\n",
    "    \n",
    "    # Save the core components subgraph\n",
    "    nx.drawing.nx_pydot.write_dot(core_G, f'core_{output_file}.dot')\n",
    "    !dot -Tpng core_{output_file}.dot -o core_{output_file}.png\n",
    "    \n",
    "    # Quick visualization using matplotlib\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    pos = nx.spring_layout(core_G)\n",
    "    nx.draw_networkx_nodes(core_G, pos, node_size=700, node_color='lightblue')\n",
    "    nx.draw_networkx_edges(core_G, pos, width=1.0, alpha=0.7)\n",
    "    nx.draw_networkx_labels(core_G, pos, font_size=8)\n",
    "    plt.title(f\"Core Components of {title}\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return core_G\n",
    "\n",
    "# Visualize core components of backend\n",
    "core_api_G = visualize_core_components(api_G, api_degree, api_between, 'api_archlens_output', 'Backend (API)')\n",
    "\n",
    "# Visualize core components of frontend\n",
    "core_frontend_G = visualize_core_components(frontend_G, frontend_degree, frontend_between, 'frontend_archlens_output', 'Frontend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Component Communication Analysis\n",
    "Let's analyze how the backend and frontend potentially communicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "def find_api_endpoints(api_dir):\n",
    "    \"\"\"Find API endpoints in the backend code\"\"\"\n",
    "    endpoints = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(api_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.py'):\n",
    "                try:\n",
    "                    with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        content = f.read()\n",
    "                    # Look for Flask routes or REST endpoints\n",
    "                    route_patterns = [\n",
    "                        r'@\\w+\\.route\\([\"\\']([^\"\\']*)[\"\\'](.*?)\\)\\s*def\\s+(\\w+)',\n",
    "                        r'@api\\.\\w+\\([\"\\']([^\"\\']*)[\"\\'](.*?)\\)\\s*def\\s+(\\w+)'\n",
    "                    ]\n",
    "                    \n",
    "                    for pattern in route_patterns:\n",
    "                        for match in re.finditer(pattern, content, re.DOTALL):\n",
    "                            endpoint = match.group(1)\n",
    "                            function_name = match.group(3)\n",
    "                            relative_file = os.path.relpath(os.path.join(root, file), api_dir)\n",
    "                            endpoints.append((endpoint, function_name, relative_file))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "    return endpoints\n",
    "\n",
    "def find_api_calls(frontend_dir):\n",
    "    \"\"\"Find API calls in the frontend code\"\"\"\n",
    "    api_calls = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(frontend_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(('.js', '.jsx', '.ts', '.tsx', '.py')):\n",
    "                try:\n",
    "                    with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        content = f.read()\n",
    "                    # Look for common HTTP request patterns (fetch, axios, etc.)\n",
    "                    patterns = [\n",
    "                        r'fetch\\([\"\\']([^\"\\']*)[\"\\'](.*?)\\)',\n",
    "                        r'axios\\.\\w+\\([\"\\']([^\"\\']*)[\"\\'](.*?)\\)',\n",
    "                        r'\\.(get|post|put|delete)\\([\"\\']([^\"\\']*)[\"\\'](.*?)\\)'\n",
    "                    ]\n",
    "                    \n",
    "                    for pattern in patterns:\n",
    "                        for match in re.finditer(pattern, content, re.DOTALL):\n",
    "                            if pattern == patterns[2]:  \n",
    "                                endpoint = match.group(2)\n",
    "                            else:\n",
    "                                endpoint = match.group(1)\n",
    "                            relative_file = os.path.relpath(os.path.join(root, file), frontend_dir)\n",
    "                            api_calls.append((endpoint, relative_file))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "    return api_calls\n",
    "\n",
    "# Execute the analysis using updated paths\n",
    "try:\n",
    "    api_endpoints = find_api_endpoints('../Data/api')\n",
    "    frontend_api_calls = find_api_calls('../Data/frontend')\n",
    "    \n",
    "    print(f\"\\nFound {len(api_endpoints)} API endpoints in the backend:\")\n",
    "    for i, (endpoint, function, file) in enumerate(api_endpoints[:10], 1):\n",
    "        print(f\"{i}. {endpoint} → {function} in {file}\")\n",
    "    if len(api_endpoints) > 10:\n",
    "        print(f\"... and {len(api_endpoints) - 10} more\")\n",
    "    \n",
    "    print(f\"\\nFound {len(frontend_api_calls)} API calls in the frontend:\")\n",
    "    for i, (endpoint, file) in enumerate(frontend_api_calls[:10], 1):\n",
    "        print(f\"{i}. {endpoint} in {file}\")\n",
    "    if len(frontend_api_calls) > 10:\n",
    "        print(f\"... and {len(frontend_api_calls) - 10} more\")\n",
    "    \n",
    "    # Save results to JSON for further analysis\n",
    "    with open('api_connections.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'backend_endpoints': api_endpoints,\n",
    "            'frontend_calls': frontend_api_calls\n",
    "        }, f, indent=2)\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing API connections: {e}\")\n",
    "    print(\"This analysis may require actual code from the Zeeguu project.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights and Recommendations\n",
    "Based on the architectural analysis, we can identify key modules and patterns in the Zeeguu system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count incoming and outgoing dependencies\n",
    "def analyze_dependencies(G):\n",
    "    dependencies = {}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        incoming = len(list(G.predecessors(node)))\n",
    "        outgoing = len(list(G.successors(node)))\n",
    "        dependencies[node] = {\n",
    "            'incoming': incoming,\n",
    "            'outgoing': outgoing,\n",
    "            'total': incoming + outgoing\n",
    "        }\n",
    "    \n",
    "    # Find highly coupled modules (both high incoming and outgoing)\n",
    "    highly_coupled = sorted(dependencies.items(), key=lambda x: x[1]['total'], reverse=True)[:5]\n",
    "    \n",
    "    # Find potential core service modules (high incoming, low outgoing)\n",
    "    core_services = sorted(dependencies.items(), key=lambda x: x[1]['incoming'] - x[1]['outgoing'], reverse=True)[:5]\n",
    "    \n",
    "    # Find implementation modules (high outgoing, low incoming)\n",
    "    implementation_modules = sorted(dependencies.items(), key=lambda x: x[1]['outgoing'] - x[1]['incoming'], reverse=True)[:5]\n",
    "    \n",
    "    print(\"\\nTop 5 Most Coupled Modules:\")\n",
    "    for node, stats in highly_coupled:\n",
    "        print(f\"{node}: {stats['incoming']} incoming, {stats['outgoing']} outgoing, {stats['total']} total\")\n",
    "    \n",
    "    print(\"\\nTop 5 Potential Core Service Modules:\")\n",
    "    for node, stats in core_services:\n",
    "        print(f\"{node}: {stats['incoming']} incoming, {stats['outgoing']} outgoing\")\n",
    "    \n",
    "    print(\"\\nTop 5 Implementation Modules:\")\n",
    "    for node, stats in implementation_modules:\n",
    "        print(f\"{node}: {stats['incoming']} incoming, {stats['outgoing']} outgoing\")\n",
    "    \n",
    "    return dependencies\n",
    "\n",
    "print(\"\\nBackend (API) Dependency Analysis:\")\n",
    "api_deps = analyze_dependencies(api_G)\n",
    "\n",
    "print(\"\\nFrontend Dependency Analysis:\")\n",
    "frontend_deps = analyze_dependencies(frontend_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Recommendations\n",
    "Based on our analysis, we can provide recommendations for improving the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(api_deps, frontend_deps):\n",
    "    print(\"Architecture Recommendations for Zeeguu:\")\n",
    "    \n",
    "    # Identify potentially problematic modules in backend\n",
    "    api_high_coupling = sorted(api_deps.items(), key=lambda x: x[1]['total'], reverse=True)[:3]\n",
    "    print(\"\\n1. Backend Refactoring Opportunities:\")\n",
    "    for node, stats in api_high_coupling:\n",
    "        print(f\"   - Consider refactoring {node} (total dependencies: {stats['total']}) into smaller, more focused modules\")\n",
    "    \n",
    "    # Identify potentially problematic modules in frontend\n",
    "    frontend_high_coupling = sorted(frontend_deps.items(), key=lambda x: x[1]['total'], reverse=True)[:3]\n",
    "    print(\"\\n2. Frontend Refactoring Opportunities:\")\n",
    "    for node, stats in frontend_high_coupling:\n",
    "        print(f\"   - Consider reviewing {node} (total dependencies: {stats['total']}) for adherence to component principles\")\n",
    "    \n",
    "    # General architectural recommendations\n",
    "    print(\"\\n3. General Architecture Recommendations:\")\n",
    "    print(\"   - Ensure clear separation between UI components and business logic in the frontend\")\n",
    "    print(\"   - Consider implementing a more formal layered architecture in the backend\")\n",
    "    print(\"   - Establish standardized API contracts between frontend and backend\")\n",
    "    print(\"   - Document key architectural decisions and patterns\")\n",
    "\n",
    "generate_recommendations(api_deps, frontend_deps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
